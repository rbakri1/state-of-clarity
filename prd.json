{
  "project": "State of Clarity",
  "branchName": "ralph/consensus-clarity-scoring",
  "description": "Replace single-judge clarity scoring with 3-agent consensus panel (Skeptic, Advocate, Generalist) evaluating 7 dimensions with discussion rounds and tiebreaker logic",
  "userStories": [
    {
      "id": "US-001",
      "title": "Define clarity scoring dimensions and types",
      "description": "As a developer, I need TypeScript types defining all 7 scoring dimensions so the scoring system is type-safe.",
      "acceptanceCriteria": [
        "Create /lib/types/clarity-scoring.ts",
        "Define 7 dimensions: firstPrinciplesCoherence, internalConsistency, evidenceQuality, accessibility, objectivity, factualAccuracy, biasDetection",
        "Each dimension has: name, weight (0-1, sum to 1), description, scoringGuidelines",
        "Define ClarityScore interface with dimension scores, overall score, critique, and confidence",
        "Define EvaluatorVerdict interface for individual evaluator output",
        "Export all types",
        "Typecheck passes"
      ],
      "priority": 1,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-002",
      "title": "Define evaluator persona types and prompts",
      "description": "As a developer, I need the 3 evaluator personas defined with distinct perspectives for political content evaluation.",
      "acceptanceCriteria": [
        "Create /lib/agents/clarity-evaluator-personas.ts",
        "Define Skeptic persona: challenges claims, flags unsupported assertions, asks 'what's the evidence?'",
        "Define Advocate persona: ensures strongest version of each position, catches strawmanning, checks balance",
        "Define Generalist persona: represents average reader, checks accessibility, flags jargon",
        "Each persona includes: name, role, systemPrompt, focusDimensions",
        "Export getEvaluatorPersona(role) and getAllEvaluatorPersonas() functions",
        "Typecheck passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-003",
      "title": "Create individual evaluator agent",
      "description": "As a developer, I need an agent that scores a brief from a single evaluator's perspective.",
      "acceptanceCriteria": [
        "Create /lib/agents/clarity-evaluator-agent.ts",
        "Function signature: evaluateBrief(brief: Brief, persona: EvaluatorPersona): Promise<EvaluatorVerdict>",
        "Returns scores for all 7 dimensions (0-10 scale)",
        "Returns written critique explaining scores",
        "Returns list of specific issues found",
        "Uses Claude Haiku for cost efficiency",
        "Completes in <5 seconds per evaluator",
        "Typecheck passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-004",
      "title": "Implement parallel evaluator execution",
      "description": "As a developer, I need all 3 evaluators to run in parallel so scoring stays fast.",
      "acceptanceCriteria": [
        "Create /lib/agents/consensus-scorer.ts",
        "Run Skeptic, Advocate, and Generalist evaluators concurrently using Promise.all",
        "Use existing retry wrapper for each evaluator",
        "Log execution time for each evaluator using execution-logger",
        "All 3 complete in <8 seconds total (parallel)",
        "Collect all 3 verdicts into ConsensusInput object",
        "Typecheck passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": ""
    },
    {
      "id": "US-005",
      "title": "Implement disagreement detection",
      "description": "As a developer, I need to detect when evaluators strongly disagree so I can trigger tiebreaker logic.",
      "acceptanceCriteria": [
        "Add function detectDisagreement(verdicts: EvaluatorVerdict[]): DisagreementResult to consensus-scorer.ts",
        "Calculate spread for each dimension (max score - min score)",
        "Calculate overall score spread across evaluators",
        "Flag disagreement if any dimension spread >2 OR overall spread >2",
        "Return: hasDisagreement, disagreeingDimensions[], maxSpread, evaluatorPositions",
        "Typecheck passes"
      ],
      "priority": 5,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-006",
      "title": "Create discussion round agent",
      "description": "As a developer, I need a discussion round where evaluators see each other's scores and can revise their assessments.",
      "acceptanceCriteria": [
        "Create /lib/agents/discussion-round-agent.ts",
        "Agent receives all 3 verdicts as input",
        "Prompt asks: Given the other evaluators perspectives, would you revise any scores? Explain your reasoning.",
        "Each evaluator can revise scores or maintain position with justification",
        "Returns revised verdicts and discussion summary",
        "Discussion round completes in <5 seconds",
        "Typecheck passes"
      ],
      "priority": 6,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-007",
      "title": "Implement tiebreaker evaluator",
      "description": "As a developer, I need a 4th evaluator that only runs when there's strong disagreement.",
      "acceptanceCriteria": [
        "Add Arbiter persona to clarity-evaluator-personas.ts: focuses on resolving specific disputed dimensions",
        "Arbiter receives: original brief, all 3 verdicts, disagreeing dimensions, discussion summary",
        "Arbiter provides definitive score for disputed dimensions with detailed reasoning",
        "Arbiter scores weighted 1.5x for disputed dimensions in final calculation",
        "Completes in <5 seconds",
        "Typecheck passes"
      ],
      "priority": 7,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-008",
      "title": "Add human review columns to briefs table",
      "description": "As a developer, I need briefs with high disagreement to be flagged for human review.",
      "acceptanceCriteria": [
        "Add needs_human_review boolean column to briefs table (default false)",
        "Add review_reason text column to briefs table (nullable)",
        "Add scoring_metadata JSONB column for dimension breakdown",
        "Create migration /lib/supabase/migrations/003_add_human_review_columns.sql",
        "Update Database interface in client.ts with new columns",
        "Typecheck passes"
      ],
      "priority": 8,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-009",
      "title": "Implement final score calculation",
      "description": "As a developer, I need to calculate the final clarity score from the consensus panel output.",
      "acceptanceCriteria": [
        "Add calculateFinalScore function to consensus-scorer.ts",
        "Calculate weighted average across all dimensions using dimension weights",
        "If no disagreement: use median of 3 evaluator scores per dimension",
        "If disagreement resolved by discussion: use post-discussion scores",
        "If tiebreaker invoked: weight Arbiter 1.5x for disputed dimensions",
        "Final score is 0-10 scale with one decimal place",
        "Return overall score, dimension breakdown, and consolidated critique",
        "Typecheck passes"
      ],
      "priority": 9,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-010",
      "title": "Aggregate critiques for refinement",
      "description": "As a developer, I need a consolidated critique that the refinement agent can act on.",
      "acceptanceCriteria": [
        "Add aggregateCritiques function to consensus-scorer.ts",
        "Merge issues from all evaluators, deduplicate similar issues",
        "Prioritize issues by: agreement (all 3 flagged it), severity (score impact), actionability",
        "Format critique as structured list: dimension, issue, suggested fix, priority",
        "Limit to top 5 issues to keep refinement focused",
        "Include specific quotes/sections that need attention",
        "Typecheck passes"
      ],
      "priority": 10,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-011",
      "title": "Integrate consensus scorer into pipeline",
      "description": "As a developer, I need the consensus scorer to replace the single-judge scorer in the brief generation pipeline.",
      "acceptanceCriteria": [
        "Update langgraph-orchestrator.ts to use consensus scorer after reconciliation",
        "Pass consensus result to refinement agent if score <8.0",
        "Flag brief for human review if tiebreaker was invoked",
        "Store final clarity score and dimension breakdown in brief record",
        "Store needs_human_review and review_reason if applicable",
        "Typecheck passes"
      ],
      "priority": 11,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-012",
      "title": "Add scoring metrics to execution logs",
      "description": "As a developer, I need detailed scoring metrics logged for analysis and prompt optimization.",
      "acceptanceCriteria": [
        "Log each evaluator's individual verdict (scores, critique, duration) to agent_execution_logs",
        "Log disagreement detection results",
        "Log discussion round output if run",
        "Log tiebreaker verdict if invoked",
        "Log final consensus score and calculation method used",
        "Typecheck passes"
      ],
      "priority": 12,
      "passes": false,
      "notes": ""
    },
    {
      "id": "US-013",
      "title": "Test consensus scoring with sample briefs",
      "description": "As a developer, I need to validate the consensus scoring system works correctly.",
      "acceptanceCriteria": [
        "Create test-consensus-scoring.ts script",
        "Test with existing sample brief (UK 4-day work week)",
        "Verify all 3 evaluators produce valid verdicts with scores 0-10",
        "Verify discussion round produces revised verdicts",
        "Test disagreement detection with artificially divergent scores",
        "Verify final score calculation is correct",
        "Print test results to console",
        "Typecheck passes"
      ],
      "priority": 13,
      "passes": false,
      "notes": ""
    }
  ]
}
